{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dadaf18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false_negative :  2\n",
      "[1.0, 3.0]\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ipynb.fs.full.test_knn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fz/zn5r8vq12nv5p23dtlr15sk40000gn/T/ipykernel_79033/3013799110.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# read this https://github.com/ipython/ipynb for importing notebooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mipynb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminkowski_distance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meuclidean_distance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosine_similarity_distance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mipynb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_knn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ipynb.fs.full.test_knn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# read this https://github.com/ipython/ipynb for importing notebooks\n",
    "from ipynb.fs.full.test_utils import f1_score, minkowski_distance, euclidean_distance, cosine_similarity_distance\n",
    "from ipynb.fs.full.test_knn import KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0c659f",
   "metadata": {},
   "source": [
    "### tuning_without_scaling\n",
    "\n",
    "        In this part, you need to try different distance functions you implemented in part 1.1 and different values of k (among 1, 3, 5, ... , 29), and find the best model with the highest f1-score on the given validation set.\n",
    "            ==> validation set : what to test funcs & k on \n",
    "            ==> try all 3 dist funcs\n",
    "            ==> k values : odd #s from 1 - 30 step 2; call function to get k values (knn.py file) here, write a func to look inside of my knn.py & call the func w/ k values\n",
    "            ==> model w/ highest f1 score : from this class, call my f1_score func\n",
    "\n",
    "        :param distance_funcs: dictionary of distance functions (key is the function name, value is the function) you need to try to calculate the distance. Make sure you loop over all distance functions for each k value.\n",
    "            ==> for k in len(self.k), \n",
    "            ==> :param dist_funcs : {'key' : value, 'key' : value, 'key' : value}\n",
    "                ==> dist_funcs = {\n",
    "                    'minkowski_distance' : minkowski_distance(point1, point2), \n",
    "                    'euclidean_distance' : euclidean_distance(point1, point2),\n",
    "                    'cosine_similarity_distance' : cosine_similarity_distance(point1, point2)\n",
    "                    }\n",
    "                ==> which dist_func returns the smallest value for this k\n",
    "            ==> return best dist_func\n",
    "            ==> starting at 1, loop through k up to 29 & get the best dist_func for each k along the w/ the best k & best f1_score\n",
    "            ==> once we get this, it will be our best model\n",
    "            \n",
    "        :param x_train: List[List[int]] training data set to train your KNN model\n",
    "            ==> features to train\n",
    "        :param y_train: List[int] training labels to train your KNN model\n",
    "            ==> labels to train [list of 0s or 1s]\n",
    "        :param x_val:  List[List[int]] validation data\n",
    "            ==> features for validation set to find f1 score & best model\n",
    "        :param y_val: List[int] validation labels\n",
    "            ==> labels for validation set to find f1 score & best model\n",
    "        \n",
    "\n",
    "        Find the best k, distance_function (its name), and model (an instance of KNN) and assign them to self.best_k,\n",
    "        self.best_distance_function, and self.best_model respectively.\n",
    "            ==> self.best_k = best k value\n",
    "            ==> self.best_distance_function = 1 of the 3 dist funcs\n",
    "            ==> self.best_model = small knn is the object of the capital KNN class [see testKNN] so return the best model by calling the KKN class\n",
    "                store max f1 score\n",
    "\n",
    "        NOTE: self.best_scaler will be None.\n",
    "\n",
    "        NOTE: When there is a tie, choose the model based on the following priorities:\n",
    "        First check the distance function:  euclidean > Minkowski > cosine_dist\n",
    "\t\t(this will also be the insertion order in \"distance_funcs\", to make things easier).\n",
    "        For the same distance function, further break tie by prioritizing a smaller k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68ce08e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperparameterTuner:\n",
    "    def __init__(self):\n",
    "        self.best_k = None # inner\n",
    "        self.best_distance_function = None # outer\n",
    "        self.best_scaler = None\n",
    "        self.best_model = None # pass d_f here; \n",
    "\n",
    "    # TODO: find parameters with the best f1 score on validation dataset\n",
    "    def tuning_without_scaling(self, distance_funcs, x_train, y_train, x_val, y_val):\n",
    "\n",
    "        max_f1 = -1\n",
    "#         if there are less than 30 points, can't have 30 neighbors\n",
    "        bound = min(30, len(x_train))\n",
    "        for name, dist_val in distance_funcs.items() :\n",
    "            print(name, \" : \", dist_val)\n",
    "        \n",
    "            for k_nums in range(1, bound, 2) :\n",
    "#                 print(\"k : \", k)\n",
    "\n",
    "                knn_m = KNN(k_nums, dist_val)\n",
    "#                 print(\"knn : \", knn)\n",
    "\n",
    "                knn_m.train(x_train, y_train)\n",
    "#                 print(\"knn.train : \",  train)\n",
    "#                 get f1 score & store them\n",
    "#                 p = predict\n",
    "                p = knn_m.predict(x_val)\n",
    "                \n",
    "#                 get the max f1\n",
    "                f1 = f1_score(y_val, p)\n",
    "\n",
    "                if f1 > max_f1 :\n",
    "#                     new max\n",
    "                    max_f1 = f1\n",
    "                    self.best_k = k_nums \n",
    "                    self.best_distance_function = name\n",
    "                    self.best_model = knn_m\n",
    "\n",
    "    # TODO: find parameters with the best f1 score on validation dataset, with normalized data\n",
    "    def tuning_with_scaling(self, distance_funcs, scaling_classes, x_train, y_train, x_val, y_val):\n",
    "        \"\"\"\n",
    "        This part is the same as \"tuning_without_scaling\", except that you also need to try two different scalers implemented in Part 1.3. More specifically, before passing the training and validation data to KNN model, apply the scalers in scaling_classes to both of them.\n",
    "\n",
    "        :param distance_funcs: dictionary of distance functions (key is the function name, value is the function) you need to try to calculate the distance. Make sure you loop over all distance functions for each k value.\n",
    "        :param scaling_classes: dictionary of scalers (key is the scaler name, value is the scaler class) you need to try to normalize your data\n",
    "        :param x_train: List[List[int]] training data set to train your KNN model\n",
    "        :param y_train: List[int] train labels to train your KNN model\n",
    "        :param x_val: List[List[int]] validation data\n",
    "        :param y_val: List[int] validation labels\n",
    "\n",
    "        Find the best k, distance_function (its name), scaler (its name), and model (an instance of KNN), and assign them to self.best_k, self.best_distance_function, best_scaler, and self.best_model respectively.\n",
    "\n",
    "        NOTE: When there is a tie, choose the model based on the following priorities:\n",
    "        First check scaler, prioritizing \"min_max_scale\" over \"normalize\" (which will also be the insertion order of scaling_classes). Then follow the same rule as in \"tuning_without_scaling\".\n",
    "        \"\"\"\n",
    "\n",
    "        # You need to assign the final values to these variables\n",
    "        self.best_k = None\n",
    "        self.best_distance_function = None\n",
    "        self.best_scaler = None\n",
    "        self.best_model = None\n",
    "#         raise NotImplementedError\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2215f9",
   "metadata": {},
   "source": [
    "# Class\n",
    "1. HyperparameterTuner\n",
    "\n",
    "## Functions\n",
    "\n",
    "1. tuning_without_scaling w/ params :\n",
    "    - distance_funcs\n",
    "    - x_train\n",
    "    - y_train\n",
    "    - x_val\n",
    "    - y_val\n",
    "2. tuning_with_scaling w/ params :\n",
    "    - distance_funcs\n",
    "    - scaling_classes\n",
    "    - x_train\n",
    "    - y_train\n",
    "    - x_val\n",
    "    - y_val\n",
    "    \n",
    "### Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2126aee7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'minkowski_distance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fz/zn5r8vq12nv5p23dtlr15sk40000gn/T/ipykernel_79033/4250718853.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m dist_funcs = {\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;34m'minkowski_distance'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mminkowski_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;34m'euclidean_distance'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0meuclidean_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;34m'cosine_similarity_distance'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mcosine_similarity_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'minkowski_distance' is not defined"
     ]
    }
   ],
   "source": [
    "# p1 = [2.0, 2.0]\n",
    "# p2 = [2.0, 2.0]\n",
    "\n",
    "p1 = [1.0, 2.0]\n",
    "p2 = [2.0, 1.0]\n",
    "point = [30.0, 5.8]\n",
    "\n",
    "dist_funcs = {\n",
    "    'minkowski_distance' : minkowski_distance(p1, p2),\n",
    "    'euclidean_distance' : euclidean_distance(p1, p2),\n",
    "    'cosine_similarity_distance' : cosine_similarity_distance(p1, p2)\n",
    "}\n",
    "# print(distance_funcs)\n",
    "\n",
    "# List[List[int]] of features\n",
    "f_train = np.array([[45.2, 6.85], [211.9, 6.13], [9.34, 32.23], [82.7, 6.3], [57.8, 3.11], [2.3, 93.0]], dtype=float)\n",
    "# print(\"f_train : \", f_train)\n",
    "\n",
    "#  List[int] of labels\n",
    "l_train = np.array([0, 1, 0, 0, 1, 1])\n",
    "# print(\"y_train : \", y_train)\n",
    "\n",
    "# List[List[int]] validation data\n",
    "f_val = np.array([[45.0, 6.85], [211.0, 6.13], [9.34, 32.23], [82.7, 6.3], [57.8, 3.11], [2.0, 93.0]], dtype=float)\n",
    "# print(\"x_val : \", x_val)\n",
    "\n",
    "# List[int] validation labels\n",
    "l_val = np.array([0, 1, 0, 0, 1, 1])\n",
    "# print(\"y_val : \", y_val)\n",
    "\n",
    "# only for f2\n",
    "# dictionary of scalers (key is the scaler name, value is the scaler class) you need to try to normalize your data\n",
    "# scaling_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f25d8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hpt = HyperparameterTuner()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3586bcf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/fz/zn5r8vq12nv5p23dtlr15sk40000gn/T/ipykernel_66636/1069223365.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TODO: find parameters with the best f1 score on validation dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuning_without_scaling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist_funcs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/fz/zn5r8vq12nv5p23dtlr15sk40000gn/T/ipykernel_66636/551330211.py\u001b[0m in \u001b[0;36mtuning_without_scaling\u001b[0;34m(self, distance_funcs, x_train, y_train, x_val, y_val)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#                 get f1 score & store them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m#                 p = predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknn_m\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#                 get the max f1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/development/csci_567_machine_learning/1-knn/my_code/test_knn.ipynb\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;34m\"        Thus, you will get N predicted label for N test data point.\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;34m\"            ==> predicted label = test data point \\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m     \u001b[0;34m\"            ==> add all 0s & 1s & that # should be the same as the total data points\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0;34m\"        This function needs to return a list of predicted labels for all test data points.\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m\"            ==> return the labels of the features of the get_k_nei output (see below)\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/development/csci_567_machine_learning/1-knn/my_code/test_knn.ipynb\u001b[0m in \u001b[0;36mget_k_neighbors\u001b[0;34m(self, point)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;34m\"    # TODO: find KNN of one point\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0;34m\"    def get_k_neighbors(self, point):\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;34m\"#         raise NotImplementedError\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object is not callable"
     ]
    }
   ],
   "source": [
    "# TODO: find parameters with the best f1 score on validation dataset\n",
    "hpt.tuning_without_scaling(dist_funcs, f_train, l_train, f_val, l_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef446eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: find parameters with the best f1 score on validation dataset, with normalized data\n",
    "hpt.tuning_with_scaling(dist_funcs, scaling_classes, f_train, l_train, f_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe71e21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
