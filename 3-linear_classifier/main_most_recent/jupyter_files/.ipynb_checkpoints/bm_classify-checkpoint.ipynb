{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "320551e9-a406-4f9b-9823-e4fc139a7d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47f0dcdb-a7a9-4871-860f-3a788ab6b827",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b379b4ef-b45a-440a-8b29-607d385d5f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toy_data_binary():\n",
    "    \"\"\"Generate a random n-class classification problem and split arrays or matrices into random train and test subsets using functions\n",
    "    \n",
    "    Functions:\n",
    "    make_classification() -- imported\n",
    "    train_test_split() -- imported\n",
    "    \n",
    "    Returns:\n",
    "    X_train, X_test, y_train, y_test -- np arrays (.\n",
    "\n",
    "    \"\"\"\n",
    "    data = make_classification(n_samples=500, \n",
    "                              n_features=2,\n",
    "                              n_informative=1, \n",
    "                              n_redundant=0, \n",
    "                              n_repeated=0, \n",
    "                              n_classes=2, \n",
    "                              n_clusters_per_class=1, \n",
    "                              class_sep=1., \n",
    "                              random_state=42)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[0], data[1], train_size=0.7, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d41d133-8e42-4cbf-abb3-b37348f852d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_toy_data = toy_data_binary()\n",
    "# binary_toy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c78b959b-a287-4ca6-a8f8-4bf6e5c6271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(true, preds):\n",
    "    return np.sum(true == preds).astype(float) / len(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bc232c8-9552-4981-b4be-660400d1972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_train(X, y, loss=\"perceptron\", w0=None, b0=None, step_size=0.5, max_iterations=1005):\n",
    "    \"\"\"Find the optimal parameters w and b for inputs X and y. Use the *average* of the gradients for all training examples multiplied by the step_size to update parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    X -- np array (training features of size N-by-D, where N is the number of training points and D is the dimensionality of features)\n",
    "    y -- np array (binary training labels of N dimensional)\n",
    "    N -- int -- (#training points, indicating the labels of training data (either 0 or 1))\n",
    "    loss -- str (loss type; either perceptron or logistic)\n",
    "    w0 -- np array (initial weight vector)\n",
    "    b0 -- scalar (initial bias term)\n",
    "    step_size -- float (learning rate)\n",
    "    max_iterations -- int (#iterations to perform gradient descent)\n",
    "\n",
    "    Returns:\n",
    "    w -- np array (D-dimensional vector, the final trained weight vector)\n",
    "    b -- scalar (the final trained bias term)\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    assert len(np.unique(y)) == 2\n",
    "    # print(N, \"training points of size: \",  D)\n",
    "    \n",
    "    \n",
    "    w = np.zeros(D)\n",
    "    if w0 is not None:\n",
    "        w = w0\n",
    "    # print(w)\n",
    "    b = 0\n",
    "    if b0 is not None:\n",
    "        b = b0\n",
    "\n",
    "    if loss == \"perceptron\":\n",
    "        ################################################\n",
    "        # TODO 1 : perform \"max_iterations\" steps of   #\n",
    "        # gradient descent with step size \"step_size\"  #\n",
    "        # to minimize perceptron loss (use -1 as the   #\n",
    "\t\t# derivative of the perceptron loss at 0)      # \n",
    "        ################################################\n",
    "        # print(X.shape, X)\n",
    "        # print(w.shape, w)\n",
    "        \n",
    "        # print(\"y : \", y)\n",
    "        # change the misclassified points from 0 to -1 with loop\n",
    "        # for i in range(len(y)):\n",
    "        #     if y[i] == 0:\n",
    "        #         y[i] = -1\n",
    "        # print(y)\n",
    "        \n",
    "        # change the misclassified points from 0 to -1 with np\n",
    "        y = np.where(y == 0, -1, 1)\n",
    "        # print(\"y : \", y)\n",
    "        \n",
    "        for i in range(0, max_iterations):\n",
    "            # print(\"epoch=\", i)\n",
    "            \n",
    "            # prediction\n",
    "            y_hat = np.add(np.dot(X, w), b)\n",
    "            # print(\"y_hat : \", y_hat.shape, y_hat)\n",
    "            \n",
    "            # classification\n",
    "            y_times_y_hat = np.multiply(y, y_hat)\n",
    "            # print(\"y_times_y_hat : \", y_times_y_hat.shape, y_times_y_hat)\n",
    "            \n",
    "            # get misclassified\n",
    "            misclassified_indicator = np.where(y_times_y_hat <= 0, -1, 1)\n",
    "            # print(\"misclassified_indicator :\", misclassified_indicator.shape, misclassified_indicator)\n",
    "            \n",
    "            # if (indicator_0_or_1 >= 1).any():\n",
    "            #     print(\"CORRECTLY CLASSIFIED POINTS\")\n",
    "            #     print(y_times_y_hat)\n",
    "            # else: \n",
    "            #     print(\"MISCLASSIFIED POINTS\")\n",
    "            #     print(y_times_y_hat)\n",
    "            # misc_i = misclassified_indicator * y\n",
    "            # print(\"misc_i : \", misc_i.shape, misc_i)\n",
    "            \n",
    "            # print(X.T.shape, y.shape)\n",
    "            gradient = np.multiply(X.T, y)        \n",
    "            # gradient = np.dot(X.T, y)\n",
    "            # print(\"gradient :\", gradient.shape, gradient)\n",
    "            # print(X, \"*\", indicator_0_or_1, \"* (\", y_hat, \"-\", y, \") =\", gradient)\n",
    "            \n",
    "            # print(w.shape, \"+\", step_size, \"*\", gradient.shape, \"*\", misclassified_indicator.shape)\n",
    "            \n",
    "            ins = np.dot(gradient, misclassified_indicator)\n",
    "            # print(\"ins : \", ins.shape, ins)\n",
    "            \n",
    "            w = np.add(w, (step_size * (ins) / N))\n",
    "            # print(\"w : \", w.shape)\n",
    "            # print(b, \"+\", step_size, \"*\", y_hat, \"*\", y.shape)\n",
    "            b = b + (step_size * np.sum(misclassified_indicator * gradient) / N)\n",
    "            # print(\"b : \", b.shape)\n",
    "           \n",
    "            # print()\n",
    "\n",
    "    elif loss == \"logistic\":\n",
    "        ################################################\n",
    "        # TODO 2 : perform \"max_iterations\" steps of   #\n",
    "        # gradient descent with step size \"step_size\"  #\n",
    "        # to minimize logistic loss                    # \n",
    "        ################################################\n",
    "\n",
    "        \n",
    "        pass\n",
    "    else:\n",
    "        raise \"Undefined loss function.\"\n",
    "\n",
    "    assert w.shape == (D,)\n",
    "    return w, b        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5a702782-ac97-4c25-a19f-4683b35bf82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_predict(X, w, b):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - X: testing features, a N-by-D numpy array, where N is the \n",
    "    number of training points and D is the dimensionality of features\n",
    "    - w: D-dimensional vector, a numpy array which is the weight \n",
    "    vector of your learned model\n",
    "    - b: scalar, which is the bias of your model\n",
    "    \n",
    "    Returns:\n",
    "    - preds: N-dimensional vector of binary predictions (either 0 or 1)\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "        \n",
    "    #############################################################\n",
    "    # TODO 4 : predict DETERMINISTICALLY (i.e. do not randomize)#\n",
    "    #############################################################\n",
    "    \n",
    "    get_pred = np.add(np.dot(X, w.T), b)\n",
    "    if (get_pred > 0).any:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "    assert preds.shape == (N,) \n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "96941ded-9c4b-4e06-96ce-3e9c25adbf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = binary_toy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "84dc446c-9d5a-4223-8997-ad75a05544b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_X_train = X_train[0:3]\n",
    "# my_y_train = y_train[0:3]\n",
    "# my_X_test = X_test[0:3]\n",
    "# my_y_test = y_test[0:3]\n",
    "\n",
    "# # print(my_X_train)\n",
    "# # print(my_y_train)\n",
    "# for loss_type in [\"perceptron\"]:\n",
    "#     my_w, my_b = binary_train(my_X_train, my_y_train, loss=loss_type)\n",
    "#     # print(my_w, my_b)\n",
    "#     # my_train_preds = binary_predict(my_X_train, my_w, my_b)\n",
    "#     # # print(my_train_preds)\n",
    "#     # my_preds = binary_predict(my_X_test, my_w, my_b)\n",
    "#     # # print(my_preds)\n",
    "#     # print(loss_type + ' train acc: %f, test acc: %f' \n",
    "#     #             %(accuracy_score(my_y_train, my_train_preds), accuracy_score(my_y_test, my_preds)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a85fa547-1563-4123-931f-0c59d5c212c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perceptron train acc: 0.514286, test acc: 0.473333\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = binary_toy_data\n",
    "# print(X_train[0:5])\n",
    "# print(y_train[0:5])\n",
    "for loss_type in [\"perceptron\"]:\n",
    "    # print(loss_type)\n",
    "    w, b = binary_train(X_train, y_train, loss=loss_type)\n",
    "    # print(w, b)\n",
    "    train_preds = binary_predict(X_train, w, b)\n",
    "    # print(train_preds)\n",
    "    preds = binary_predict(X_test, w, b)\n",
    "    # print(preds)\n",
    "    print(loss_type + ' train acc: %f, test acc: %f' \n",
    "                %(accuracy_score(y_train, train_preds), accuracy_score(y_test, preds)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2c8f1bd-c2f2-4579-98be-7fd96e9d7f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_X_train = np.array([\n",
    "#             [2.7810836,2.550537003,0],\n",
    "#             [1.465489372,2.362125076,0],\n",
    "#             [3.396561688,4.400293529,0],\n",
    "#             [1.38807019,1.850220317,0],\n",
    "#             [3.06407232,3.005305973,0],\n",
    "#             [7.627531214,2.759262235,1],\n",
    "#             [5.332441248,2.088626775,1],\n",
    "#             [6.922596716,1.77106367,1],\n",
    "#             [8.675418651,-0.242068655,1],\n",
    "#             [7.673756466,3.508563011,1]\n",
    "#             ])\n",
    "\n",
    "# my_w_train = np.array([-0.1, 0.20653640140000007, -0.23418117710000003])\n",
    "\n",
    "# my_y_train = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n",
    "\n",
    "# for loss_type in [\"perceptron\"]:\n",
    "#     my_w, my_b = binary_train(my_X_train, my_y_train, loss=loss_type, w0=my_w_train)\n",
    "\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4345e65b-e714-4e6f-a29b-828effd6b002",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. USC CSCI-567 Machine Learning\n",
    "2. [make_classification](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html#sklearn-datasets-make-classification) Documentation\n",
    "3. [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split) Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4766ecdf-0cc3-4d86-bb74-7ce1f14fc0d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
