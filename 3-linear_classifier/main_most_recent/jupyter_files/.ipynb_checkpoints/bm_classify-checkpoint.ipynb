{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f595120-f83b-4bc0-99a5-018ae5c7d32f",
   "metadata": {},
   "source": [
    "# Imports + Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "320551e9-a406-4f9b-9823-e4fc139a7d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47f0dcdb-a7a9-4871-860f-3a788ab6b827",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import sklearn.datasets as skl_datasets\n",
    "import sklearn.model_selection as skl_model_selection\n",
    "\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b379b4ef-b45a-440a-8b29-607d385d5f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toy_data_binary():\n",
    "    \"\"\"Generate a random n-class classification problem and split arrays or matrices into random train and test subsets using functions\n",
    "    \n",
    "    Returns:\n",
    "    X_train, X_test, y_train, y_test -- np arrays\n",
    "\n",
    "    \"\"\"\n",
    "    data = skl_datasets.make_classification(n_samples=500, \n",
    "                              n_features=2,\n",
    "                              n_informative=1, \n",
    "                              n_redundant=0, \n",
    "                              n_repeated=0, \n",
    "                              n_classes=2, \n",
    "                              n_clusters_per_class=1, \n",
    "                              class_sep=1., \n",
    "                              random_state=42)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = skl_model_selection.train_test_split(data[0], data[1], train_size=0.7, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d41d133-8e42-4cbf-abb3-b37348f852d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_toy_data = toy_data_binary()\n",
    "# binary_toy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c724066f-a65a-48f4-b979-2d7a80ad6b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moon_dataset():\n",
    "    \"\"\"Generate two interleaving half circles (per the documentation)\n",
    "    \n",
    "    Returns:\n",
    "    X_train, X_test, y_train, y_test -- np arrays\n",
    "    \n",
    "    \"\"\"\n",
    "    data = skl_datasets.make_moons(n_samples=500, shuffle=True, noise=0.2, random_state=42)\n",
    "    X_train, X_test, y_train, y_test = skl_model_selection.train_test_split(data[0], data[1], train_size=0.7, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8f32fce-e8b6-458d-94c4-a224c6d72c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "moon_data = toy_data_binary()\n",
    "# moon_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c13442f-71df-4b2c-9c6e-5ed2f152b9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hand-written digits data\n",
    "def data_loader_mnist(dataset='mnist_subset.json'):\n",
    "    # This function reads the MNIST data and separate it into train, val, and test set\n",
    "    with open(dataset, 'r') as f:\n",
    "        data_set = json.load(f)\n",
    "    train_set, valid_set, test_set = data_set['train'], data_set['valid'], data_set['test']\n",
    "    \n",
    "    return np.asarray(train_set[0]), \\\n",
    "          np.asarray(test_set[0]), \\\n",
    "          np.asarray(train_set[1]), \\\n",
    "          np.asarray(test_set[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a70603c8-90aa-42c2-8654-3788a4449dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = data_loader_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f75020b-5d99-4a40-af29-70f27514a1ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d6c3a53-1096-433f-964e-f7b02767be51",
   "metadata": {},
   "source": [
    "# TODOs\n",
    "\n",
    "1. [x] perform \"max_iterations\" steps of gradient descent with step size \"step_size\" to minimize perceptron loss (use -1 as the derivative of the perceptron loss at 0) \n",
    "2. [x] perform \"max_iterations\" steps of gradient descent with step size \"step_size\" to minimize logistic loss \n",
    "3. [x] fill in the sigmoid function\n",
    "4. [x] predict DETERMINISTICALLY (i.e. do not randomize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c78b959b-a287-4ca6-a8f8-4bf6e5c6271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(true, preds):\n",
    "    return np.sum(true == preds).astype(float) / len(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d3e7aab-7369-418e-9839-a7b317c938f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    z -- a np array or a float number\n",
    "    \n",
    "    Returns:\n",
    "    value -- a numpy array or a float number after applying the sigmoid function 1/(1+exp(-z))\n",
    "    \"\"\"\n",
    "\n",
    "    ############################################\n",
    "    # TODO 3 : fill in the sigmoid function    #\n",
    "    ############################################\n",
    "    # print(\"z:\", z)\n",
    "    sigm = 1 / (1 + np.exp(-z))\n",
    "    # print(\"sigm:\", sigm)\n",
    "    return sigm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a702782-ac97-4c25-a19f-4683b35bf82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_predict(X, w, b):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    X -- testing features, a N-by-D np array, where N is the number of training points and D is the dimensionality of features\n",
    "    w -- D-dimensional vector, a np array which is the weight vector of your learned model\n",
    "    b -- scalar, which is the bias of your model\n",
    "    \n",
    "    Returns:\n",
    "    preds -- N-dimensional vector of binary predictions (either 0 or 1)\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "        \n",
    "    #############################################################\n",
    "    # TODO 4 : predict DETERMINISTICALLY (i.e. do not randomize) #\n",
    "    #############################################################\n",
    "    \n",
    "    store_preds = []\n",
    "    make_preds = np.add(np.dot(X, w.T), b)\n",
    "    \n",
    "    for i in range(len(make_preds)):\n",
    "        # print(\"make_preds[i]: \", make_preds[i])\n",
    "        if (make_preds[i] > 0):\n",
    "            store_preds.append(1)    \n",
    "        else:\n",
    "            store_preds.append(0)\n",
    "            \n",
    "    preds = np.array(store_preds)\n",
    "    assert preds.shape == (N,) \n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bc232c8-9552-4981-b4be-660400d1972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_train(X, y, loss=\"perceptron\", w0=None, b0=None, step_size=0.5, max_iterations=1000):\n",
    "    \"\"\"Find the optimal parameters w and b for inputs X and y. Use the *average* of the gradients for all training examples multiplied by the step_size to update parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    X -- np array (training features of size N-by-D, where N is the number of training points and D is the dimensionality of features)\n",
    "    y -- np array (binary training labels of N dimensional)\n",
    "    N -- int -- (#training points, indicating the labels of training data (either 0 or 1))\n",
    "    loss -- str (loss type; either perceptron or logistic)\n",
    "    w0 -- np array (initial weight vector)\n",
    "    b0 -- scalar (initial bias term)\n",
    "    step_size -- float (learning rate)\n",
    "    max_iterations -- int (#iterations to perform gradient descent)\n",
    "    \n",
    "    Functions:\n",
    "    sigmoid -- float (sigmoid value between or equal to 0 and 1)\n",
    "\n",
    "    Returns:\n",
    "    w -- np array (D-dimensional vector, the final trained weight vector)\n",
    "    b -- scalar (the final trained bias term)\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    assert len(np.unique(y)) == 2\n",
    "        \n",
    "    w = np.zeros(D)\n",
    "    if w0 is not None:\n",
    "        w = w0\n",
    "        \n",
    "    b = 0\n",
    "    if b0 is not None:\n",
    "        b = b0\n",
    "\n",
    "    if loss == \"perceptron\":\n",
    "        ################################################\n",
    "        # TODO 1 : perform \"max_iterations\" steps of   #\n",
    "        # gradient descent with step size \"step_size\"  #\n",
    "        # to minimize perceptron loss (use -1 as the   #\n",
    "\t\t# derivative of the perceptron loss at 0)      # \n",
    "        ################################################\n",
    "        \n",
    "        # the labels are either 0 or 1, instead of -1 or +1, so\n",
    "        # change 0 to -1 for the misclassified points\n",
    "        y = np.where(y == 0, -1, 1)\n",
    "        \n",
    "        for i in range(0, max_iterations):\n",
    "            \n",
    "            \n",
    "            # print(y_hat)\n",
    "            \n",
    "            # classify the prediction\n",
    "            # y_times_y_hat = np.multiply(y, y_hat)\n",
    "            # print(y_times_y_hat)\n",
    "            \n",
    "            for i in range(N):\n",
    "                # make a prediction\n",
    "                y_hat = np.sign(np.dot(X[i], w) +  b)\n",
    "                if y_hat != y[i]:\n",
    "                    \n",
    "                    # gradient of prediction\n",
    "                    gradient = y[i] * X[i]\n",
    "                    \n",
    "                    # update w and b\n",
    "                    w = w + step_size * gradient\n",
    "                    b = b + step_size * y[i]\n",
    "\n",
    "    elif loss == \"logistic\":\n",
    "        ################################################\n",
    "        # TODO 2 : perform \"max_iterations\" steps of   #\n",
    "        # gradient descent with step size \"step_size\"  #\n",
    "        # to minimize logistic loss                    # \n",
    "        ################################################\n",
    "        \n",
    "        # replace 0 with -1 as both 0 and -1 are misclassified points\n",
    "        # y = np.where(y == 0, -1, 1)\n",
    "        \n",
    "        for i in range (0, max_iterations):\n",
    "            \n",
    "            # make a prediction\n",
    "            y_hat = np.add(np.dot(X, w), b)\n",
    "            # print(\"y_hat:\", y_hat)\n",
    "\n",
    "            # predicted probabilities\n",
    "            prob_of_y_hat = sigmoid(y_hat)\n",
    "            # print(\"prob_of_y_hat:\", prob_of_y_hat)\n",
    "            \n",
    "            difference_in_prob_pred_and_true = prob_of_y_hat - y\n",
    "            # print(\"difference_in_prob_pred_and_true:\", difference_in_prob_pred_and_true)\n",
    "            \n",
    "            # gradient of predicted probability\n",
    "            gradient = np.dot(X.T, difference_in_prob_pred_and_true)\n",
    "            # print(\"gradient:\", gradient)\n",
    "            \n",
    "            # gradient of predicted probability wrt w\n",
    "            gradient_w = 1 / N * gradient\n",
    "            # print(\"gradient_w:\", gradient_w)\n",
    "            \n",
    "            # gradient of predicted probability wrt b\n",
    "            gradient_b = 1 / N * np.sum(difference_in_prob_pred_and_true)\n",
    "            # print(\"gradient_b:\", gradient_b)\n",
    "            \n",
    "            # update w and b\n",
    "            w = w - step_size * gradient_w\n",
    "            # print(w)\n",
    "            b = b - step_size * gradient_b\n",
    "            # print(b)\n",
    "            \n",
    "            # print()\n",
    "            \n",
    "    else:\n",
    "        raise \"Undefined loss function.\"\n",
    "\n",
    "    assert w.shape == (D,)\n",
    "    return w, b        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2c8f1bd-c2f2-4579-98be-7fd96e9d7f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_binary(binary_toy_data, moon_data, mnist_data):\n",
    "    \n",
    "    datasets = [(binary_toy_data, 'Synthetic data'), \n",
    "                (moon_data, 'Two Moon data'),\n",
    "                (mnist_data, 'Binarized MNIST data')]\n",
    "\n",
    "    for data, name in datasets:\n",
    "        print(name)\n",
    "        X_train, X_test, y_train, y_test = data\n",
    "        \n",
    "        if name == 'Binarized MNIST data':\n",
    "            y_train = [0 if yi < 5 else 1 for yi in y_train]\n",
    "            y_test = [0 if yi < 5 else 1 for yi in y_test]\n",
    "            y_train = np.asarray(y_train)\n",
    "            y_test = np.asarray(y_test)\n",
    "        \n",
    "        for loss_type in [\"perceptron\", \"logistic\"]:\n",
    "            \n",
    "            w, b = binary_train(X_train, y_train, loss=loss_type)\n",
    "            train_preds = binary_predict(X_train, w, b)\n",
    "            preds = binary_predict(X_test, w, b)\n",
    "            print(loss_type + ' train acc: %f, test acc: %f' \n",
    "                        %(accuracy_score(y_train, train_preds), accuracy_score(y_test, preds)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc225d41-d6b3-4dcf-85c6-337c198573f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic data\n",
      "perceptron train acc: 0.994286, test acc: 1.000000\n",
      "logistic train acc: 0.994286, test acc: 1.000000\n",
      "\n",
      "Two Moon data\n",
      "perceptron train acc: 0.994286, test acc: 1.000000\n",
      "logistic train acc: 0.994286, test acc: 1.000000\n",
      "\n",
      "Binarized MNIST data\n",
      "perceptron train acc: 0.817000, test acc: 0.777000\n",
      "logistic train acc: 0.871000, test acc: 0.834000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_binary(binary_toy_data, moon_data, mnist_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4345e65b-e714-4e6f-a29b-828effd6b002",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. CLASS: [USC CSCI-567 Machine Learning](https://haipeng-luo.net/courses/CSCI567/2021_fall/index.html) by Haipeng Luo\n",
    "2. WEBSITE: [make_classification](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html#sklearn-datasets-make-classification) scikit-learn Documentation\n",
    "3. WEBSITE: [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split) scikit-learn Documentation\n",
    "4. PAPER: [Classification Notes](https://detraviousjbrinkley.notion.site/Classification-aka-Categorical-519984d18e3748c287b94d318e1fd0aa) by Detravious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4766ecdf-0cc3-4d86-bb74-7ce1f14fc0d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
