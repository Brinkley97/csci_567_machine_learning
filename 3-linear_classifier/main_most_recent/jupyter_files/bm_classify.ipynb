{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "320551e9-a406-4f9b-9823-e4fc139a7d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Users/brinkley97/opt/anaconda3/envs/time_series_basics/lib/python3.7/site-packages (1.0.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/brinkley97/opt/anaconda3/envs/time_series_basics/lib/python3.7/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /Users/brinkley97/opt/anaconda3/envs/time_series_basics/lib/python3.7/site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /Users/brinkley97/opt/anaconda3/envs/time_series_basics/lib/python3.7/site-packages (from scikit-learn) (1.21.6)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/brinkley97/opt/anaconda3/envs/time_series_basics/lib/python3.7/site-packages (from scikit-learn) (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47f0dcdb-a7a9-4871-860f-3a788ab6b827",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b379b4ef-b45a-440a-8b29-607d385d5f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toy_data_binary():\n",
    "    \"\"\"Generate a random n-class classification problem and split arrays or matrices into random train and test subsets using functions\n",
    "    \n",
    "    Functions:\n",
    "    make_classification() -- imported\n",
    "    train_test_split() -- imported\n",
    "    \n",
    "    Returns:\n",
    "    X_train, X_test, y_train, y_test -- np arrays (.\n",
    "\n",
    "    \"\"\"\n",
    "    data = make_classification(n_samples=500, \n",
    "                              n_features=2,\n",
    "                              n_informative=1, \n",
    "                              n_redundant=0, \n",
    "                              n_repeated=0, \n",
    "                              n_classes=2, \n",
    "                              n_clusters_per_class=1, \n",
    "                              class_sep=1., \n",
    "                              random_state=42)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[0], data[1], train_size=0.7, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7d41d133-8e42-4cbf-abb3-b37348f852d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_toy_data = toy_data_binary()\n",
    "# binary_toy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "2bc232c8-9552-4981-b4be-660400d1972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_train(X, y, loss=\"perceptron\", w0=None, b0=None, step_size=0.5, max_iterations=5):\n",
    "    \"\"\"Find the optimal parameters w and b for inputs X and y. Use the *average* of the gradients for all training examples multiplied by the step_size to update parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    X -- np array (training features of size N-by-D, where N is the number of training points and D is the dimensionality of features)\n",
    "    y -- np array (binary training labels of N dimensional)\n",
    "    N -- int -- (#training points, indicating the labels of training data (either 0 or 1))\n",
    "    loss -- str (loss type; either perceptron or logistic)\n",
    "    w0 -- np array (initial weight vector)\n",
    "    b0 -- scalar (initial bias term)\n",
    "    step_size -- float (learning rate)\n",
    "    max_iterations -- int (#iterations to perform gradient descent)\n",
    "\n",
    "    Returns:\n",
    "    w -- np array (D-dimensional vector, the final trained weight vector)\n",
    "    b -- scalar (the final trained bias term)\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    assert len(np.unique(y)) == 2\n",
    "    # print(N, \"training points of size: \",  D)\n",
    "\n",
    "    w = np.zeros(D)\n",
    "    if w0 is not None:\n",
    "        w = w0\n",
    "    \n",
    "    b = 0\n",
    "    if b0 is not None:\n",
    "        b = b0\n",
    "\n",
    "    if loss == \"perceptron\":\n",
    "        ################################################\n",
    "        # TODO 1 : perform \"max_iterations\" steps of   #\n",
    "        # gradient descent with step size \"step_size\"  #\n",
    "        # to minimize perceptron loss (use -1 as the   #\n",
    "\t\t# derivative of the perceptron loss at 0)      # \n",
    "        ################################################\n",
    "        # print(X.shape, X)\n",
    "        # print(w.shape, w)\n",
    "        \n",
    "        # print(\"y : \", y)\n",
    "        # change the misclassified points from 0 to -1 with loop\n",
    "        for i in range(len(y)):\n",
    "            if y[i] == 0:\n",
    "                y[i] = -1\n",
    "        # print(y)\n",
    "        # change the misclassified points from 0 to -1 with np\n",
    "        # y = np.where(y == 0, -1, 1)\n",
    "        # print(\"y : \", y)\n",
    "        \n",
    "        for i in range(0, max_iterations):\n",
    "            # print(y[i].shape, w.shape, X[i].shape, b)\n",
    "            # print(X, \"*\", w)\n",
    "            X_w_b = np.dot(X, w) + b\n",
    "            # print(\"X_w_b : \", X_w_b.shape, X_w_b)\n",
    "            \n",
    "            # print(y.shape, \"*\", X_w_b.shape, \"+\", np.shape(b))\n",
    "            # print(y, \"*\", X_w_b, \"+\", b)\n",
    "            y_X_w_b = np.dot(y, X_w_b)\n",
    "            # print(\"y_X_w_b : \", y_X_w_b.shape, y_X_w_b)\n",
    "            \n",
    "            print(y_X_w_b, \"vs\", y)\n",
    "            if y_X_w_b <= 0:\n",
    "                # print(\"MISCLASSIFIED\")\n",
    "                indicator = 1\n",
    "                i_y = np.multiply(indicator, y)\n",
    "                # print(\"i_y : \", i_y.shape)\n",
    "                i_y_X = np.dot(i_y, X)\n",
    "                # print(\"i_y_X : \", i_y_X.shape, i_y_X)\n",
    "\n",
    "                w = w + step_size * i_y_X / N\n",
    "                # print(\"w : \", w)\n",
    "                b = b + np.sum(step_size * i_y / N)\n",
    "                # print(\"b : \", b)\n",
    "            else:\n",
    "                # print(\"CORRECTLY CLASSIFIED\")\n",
    "                indicator = 0\n",
    "            \n",
    "            \n",
    "            # indicator = np.where(y_X_w_b <= 0, 1, 0)\n",
    "            print(indicator)\n",
    "            \n",
    "            \n",
    "           \n",
    "            print()\n",
    "\n",
    "    elif loss == \"logistic\":\n",
    "        ################################################\n",
    "        # TODO 2 : perform \"max_iterations\" steps of   #\n",
    "        # gradient descent with step size \"step_size\"  #\n",
    "        # to minimize logistic loss                    # \n",
    "        ################################################\n",
    "\n",
    "        \n",
    "        pass\n",
    "    else:\n",
    "        raise \"Undefined loss function.\"\n",
    "\n",
    "    assert w.shape == (D,)\n",
    "    return w, b        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "96941ded-9c4b-4e06-96ce-3e9c25adbf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = binary_toy_data\n",
    "# print(X_train[0:5])\n",
    "# print(y_train[0:5])\n",
    "# for loss_type in [\"perceptron\", \"logistic\"]:\n",
    "#     print(loss_type)\n",
    "    # w, b = binary_train(X_train, y_train, loss=loss_type)\n",
    "    # train_preds = binary_predict(X_train, w, b)\n",
    "    # preds = binary_predict(X_test, w, b)\n",
    "    # print(loss_type + ' train acc: %f, test acc: %f' \n",
    "                # %(accuracy_score(y_train, train_preds), accuracy_score(y_test, preds)))\n",
    "    # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "84dc446c-9d5a-4223-8997-ad75a05544b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 vs [ 1 -1  1]\n",
      "MISCLASSIFIED\n",
      "1\n",
      "\n",
      "1.7850477412951153 vs [ 1 -1  1]\n",
      "CORRECTLY CLASSIFIED\n",
      "0\n",
      "\n",
      "1.7850477412951153 vs [ 1 -1  1]\n",
      "CORRECTLY CLASSIFIED\n",
      "0\n",
      "\n",
      "1.7850477412951153 vs [ 1 -1  1]\n",
      "CORRECTLY CLASSIFIED\n",
      "0\n",
      "\n",
      "1.7850477412951153 vs [ 1 -1  1]\n",
      "CORRECTLY CLASSIFIED\n",
      "0\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_X_train = X_train[0:3]\n",
    "my_y_train = y_train[0:3]\n",
    "# print(my_X_train)\n",
    "# print(my_y_train)\n",
    "for loss_type in [\"perceptron\", \"logistic\"]:\n",
    "    my_w, my_b = binary_train(my_X_train, my_y_train, loss=loss_type)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c2c8f1bd-c2f2-4579-98be-7fd96e9d7f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,) * (3,)\n",
      "[1 1 1] * [2 2 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = np.array([1, 1, 1])\n",
    "b = np.array([2, 2, 2])\n",
    "\n",
    "print(d.shape, \"*\", b.shape)\n",
    "print(d, \"*\", b)\n",
    "# j = np.dot(d, b)\n",
    "j = d * b\n",
    "j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4345e65b-e714-4e6f-a29b-828effd6b002",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. USC CSCI-567 Machine Learning\n",
    "2. [make_classification](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html#sklearn-datasets-make-classification) Documentation\n",
    "3. [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split) Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4766ecdf-0cc3-4d86-bb74-7ce1f14fc0d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
