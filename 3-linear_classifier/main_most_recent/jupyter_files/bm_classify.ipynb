{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "320551e9-a406-4f9b-9823-e4fc139a7d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47f0dcdb-a7a9-4871-860f-3a788ab6b827",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b379b4ef-b45a-440a-8b29-607d385d5f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toy_data_binary():\n",
    "    \"\"\"Generate a random n-class classification problem and split arrays or matrices into random train and test subsets using functions\n",
    "    \n",
    "    Functions:\n",
    "    make_classification() -- imported\n",
    "    train_test_split() -- imported\n",
    "    \n",
    "    Returns:\n",
    "    X_train, X_test, y_train, y_test -- np arrays (.\n",
    "\n",
    "    \"\"\"\n",
    "    data = make_classification(n_samples=500, \n",
    "                              n_features=2,\n",
    "                              n_informative=1, \n",
    "                              n_redundant=0, \n",
    "                              n_repeated=0, \n",
    "                              n_classes=2, \n",
    "                              n_clusters_per_class=1, \n",
    "                              class_sep=1., \n",
    "                              random_state=42)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[0], data[1], train_size=0.7, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d41d133-8e42-4cbf-abb3-b37348f852d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_toy_data = toy_data_binary()\n",
    "# binary_toy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c78b959b-a287-4ca6-a8f8-4bf6e5c6271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(true, preds):\n",
    "    return np.sum(true == preds).astype(float) / len(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a702782-ac97-4c25-a19f-4683b35bf82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_predict(X, w, b):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - X: testing features, a N-by-D numpy array, where N is the \n",
    "    number of training points and D is the dimensionality of features\n",
    "    - w: D-dimensional vector, a numpy array which is the weight \n",
    "    vector of your learned model\n",
    "    - b: scalar, which is the bias of your model\n",
    "    \n",
    "    Returns:\n",
    "    - preds: N-dimensional vector of binary predictions (either 0 or 1)\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "        \n",
    "    #############################################################\n",
    "    # TODO 4 : predict DETERMINISTICALLY (i.e. do not randomize)#\n",
    "    #############################################################\n",
    "    \n",
    "    get_pred = np.add(np.dot(X, w.T), b)\n",
    "    if (get_pred > 0).any:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "    assert preds.shape == (N,) \n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bc232c8-9552-4981-b4be-660400d1972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_train(X, y, loss=\"perceptron\", w0=None, b0=None, step_size=0.5, max_iterations=1005):\n",
    "    \"\"\"Find the optimal parameters w and b for inputs X and y. Use the *average* of the gradients for all training examples multiplied by the step_size to update parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    X -- np array (training features of size N-by-D, where N is the number of training points and D is the dimensionality of features)\n",
    "    y -- np array (binary training labels of N dimensional)\n",
    "    N -- int -- (#training points, indicating the labels of training data (either 0 or 1))\n",
    "    loss -- str (loss type; either perceptron or logistic)\n",
    "    w0 -- np array (initial weight vector)\n",
    "    b0 -- scalar (initial bias term)\n",
    "    step_size -- float (learning rate)\n",
    "    max_iterations -- int (#iterations to perform gradient descent)\n",
    "\n",
    "    Returns:\n",
    "    w -- np array (D-dimensional vector, the final trained weight vector)\n",
    "    b -- scalar (the final trained bias term)\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    assert len(np.unique(y)) == 2\n",
    "        \n",
    "    w = np.zeros(D)\n",
    "    if w0 is not None:\n",
    "        w = w0\n",
    "        \n",
    "    b = 0\n",
    "    if b0 is not None:\n",
    "        b = b0\n",
    "\n",
    "    if loss == \"perceptron\":\n",
    "        ################################################\n",
    "        # TODO 1 : perform \"max_iterations\" steps of   #\n",
    "        # gradient descent with step size \"step_size\"  #\n",
    "        # to minimize perceptron loss (use -1 as the   #\n",
    "\t\t# derivative of the perceptron loss at 0)      # \n",
    "        ################################################\n",
    "        \n",
    "        # the labels are either 0 or 1, instead of -1 or +1, so\n",
    "        # change 0 to -1 for the misclassified points\n",
    "        y = np.where(y == 0, -1, 1)\n",
    "        \n",
    "        for i in range(0, max_iterations):\n",
    "            \n",
    "            # make a prediction\n",
    "            y_hat = np.add(np.dot(X, w), b)\n",
    "            \n",
    "            # classify the prediction\n",
    "            y_times_y_hat = np.multiply(y, y_hat)\n",
    "            \n",
    "            # get the misclassified predictions\n",
    "            misclassified_indicator = np.where(y_times_y_hat <= 0, -1, 1)\n",
    "            \n",
    "            # gradient\n",
    "            gradient = np.multiply(X.T, y)        \n",
    "            \n",
    "            # update w and b\n",
    "            w = np.add(w, (step_size * np.dot(gradient, misclassified_indicator) / N))\n",
    "            b = b + (step_size * np.sum(misclassified_indicator * gradient) / N)\n",
    "            \n",
    "            # print()\n",
    "\n",
    "    elif loss == \"logistic\":\n",
    "        ################################################\n",
    "        # TODO 2 : perform \"max_iterations\" steps of   #\n",
    "        # gradient descent with step size \"step_size\"  #\n",
    "        # to minimize logistic loss                    # \n",
    "        ################################################\n",
    "\n",
    "        \n",
    "        pass\n",
    "    else:\n",
    "        raise \"Undefined loss function.\"\n",
    "\n",
    "    assert w.shape == (D,)\n",
    "    return w, b        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3e7aab-7369-418e-9839-a7b317c938f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - z: a numpy array or a float number\n",
    "    Returns:\n",
    "    - value: a numpy array or a float number after applying the sigmoid function 1/(1+exp(-z)).\n",
    "    \"\"\"\n",
    "\n",
    "    ############################################\n",
    "    # TODO 3 : fill in the sigmoid function    #\n",
    "    ############################################\n",
    "    value = 1 / (1 + np.exp(-z))\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96941ded-9c4b-4e06-96ce-3e9c25adbf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = binary_toy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84dc446c-9d5a-4223-8997-ad75a05544b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_X_train = X_train[0:3]\n",
    "# my_y_train = y_train[0:3]\n",
    "# my_X_test = X_test[0:3]\n",
    "# my_y_test = y_test[0:3]\n",
    "\n",
    "# # print(my_X_train)\n",
    "# # print(my_y_train)\n",
    "# for loss_type in [\"perceptron\"]:\n",
    "#     my_w, my_b = binary_train(my_X_train, my_y_train, loss=loss_type)\n",
    "#     # print(my_w, my_b)\n",
    "#     # my_train_preds = binary_predict(my_X_train, my_w, my_b)\n",
    "#     # # print(my_train_preds)\n",
    "#     # my_preds = binary_predict(my_X_test, my_w, my_b)\n",
    "#     # # print(my_preds)\n",
    "#     # print(loss_type + ' train acc: %f, test acc: %f' \n",
    "#     #             %(accuracy_score(my_y_train, my_train_preds), accuracy_score(my_y_test, my_preds)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a85fa547-1563-4123-931f-0c59d5c212c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perceptron train acc: 0.514286, test acc: 0.473333\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = binary_toy_data\n",
    "# print(X_train[0:5])\n",
    "# print(y_train[0:5])\n",
    "for loss_type in [\"perceptron\"]:\n",
    "    # print(loss_type)\n",
    "    w, b = binary_train(X_train, y_train, loss=loss_type)\n",
    "    # print(w, b)\n",
    "    train_preds = binary_predict(X_train, w, b)\n",
    "    # print(train_preds)\n",
    "    preds = binary_predict(X_test, w, b)\n",
    "    # print(preds)\n",
    "    print(loss_type + ' train acc: %f, test acc: %f' \n",
    "                %(accuracy_score(y_train, train_preds), accuracy_score(y_test, preds)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c8f1bd-c2f2-4579-98be-7fd96e9d7f10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4345e65b-e714-4e6f-a29b-828effd6b002",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. USC CSCI-567 Machine Learning\n",
    "2. [make_classification](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html#sklearn-datasets-make-classification) Documentation\n",
    "3. [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split) Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4766ecdf-0cc3-4d86-bb74-7ce1f14fc0d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
