{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "320551e9-a406-4f9b-9823-e4fc139a7d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47f0dcdb-a7a9-4871-860f-3a788ab6b827",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b379b4ef-b45a-440a-8b29-607d385d5f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toy_data_binary():\n",
    "    \"\"\"Generate a random n-class classification problem and split arrays or matrices into random train and test subsets using functions\n",
    "    \n",
    "    Functions:\n",
    "    make_classification() -- imported\n",
    "    train_test_split() -- imported\n",
    "    \n",
    "    Returns:\n",
    "    X_train, X_test, y_train, y_test -- np arrays (.\n",
    "\n",
    "    \"\"\"\n",
    "    data = make_classification(n_samples=500, \n",
    "                              n_features=2,\n",
    "                              n_informative=1, \n",
    "                              n_redundant=0, \n",
    "                              n_repeated=0, \n",
    "                              n_classes=2, \n",
    "                              n_clusters_per_class=1, \n",
    "                              class_sep=1., \n",
    "                              random_state=42)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data[0], data[1], train_size=0.7, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d41d133-8e42-4cbf-abb3-b37348f852d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_toy_data = toy_data_binary()\n",
    "# binary_toy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c78b959b-a287-4ca6-a8f8-4bf6e5c6271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(true, preds):\n",
    "    return np.sum(true == preds).astype(float) / len(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2bc232c8-9552-4981-b4be-660400d1972d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_train(X, y, loss=\"perceptron\", w0=None, b0=None, step_size=0.5, max_iterations=1005):\n",
    "    \"\"\"Find the optimal parameters w and b for inputs X and y. Use the *average* of the gradients for all training examples multiplied by the step_size to update parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    X -- np array (training features of size N-by-D, where N is the number of training points and D is the dimensionality of features)\n",
    "    y -- np array (binary training labels of N dimensional)\n",
    "    N -- int -- (#training points, indicating the labels of training data (either 0 or 1))\n",
    "    loss -- str (loss type; either perceptron or logistic)\n",
    "    w0 -- np array (initial weight vector)\n",
    "    b0 -- scalar (initial bias term)\n",
    "    step_size -- float (learning rate)\n",
    "    max_iterations -- int (#iterations to perform gradient descent)\n",
    "\n",
    "    Returns:\n",
    "    w -- np array (D-dimensional vector, the final trained weight vector)\n",
    "    b -- scalar (the final trained bias term)\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    assert len(np.unique(y)) == 2\n",
    "    print(N, \"training points of size: \",  D)\n",
    "    \n",
    "    \n",
    "    w = np.zeros(D)\n",
    "    if w0 is not None:\n",
    "        w = w0\n",
    "    # print(w)\n",
    "    b = 0\n",
    "    if b0 is not None:\n",
    "        b = b0\n",
    "\n",
    "    if loss == \"perceptron\":\n",
    "        ################################################\n",
    "        # TODO 1 : perform \"max_iterations\" steps of   #\n",
    "        # gradient descent with step size \"step_size\"  #\n",
    "        # to minimize perceptron loss (use -1 as the   #\n",
    "\t\t# derivative of the perceptron loss at 0)      # \n",
    "        ################################################\n",
    "        # print(X.shape, X)\n",
    "        # print(w.shape, w)\n",
    "        \n",
    "        # print(\"y : \", y)\n",
    "        # change the misclassified points from 0 to -1 with loop\n",
    "        for i in range(len(y)):\n",
    "            if y[i] == 0:\n",
    "                y[i] = -1\n",
    "        # print(y)\n",
    "        \n",
    "        # change the misclassified points from 0 to -1 with np\n",
    "        # y = np.where(y == 0, -1, 1)\n",
    "        # print(\"y : \", y)\n",
    "        \n",
    "        for i in range(0, max_iterations):\n",
    "            # print(\"epoch=\", i)\n",
    "            \n",
    "            # prediction\n",
    "            y_hat = np.dot(X, w) + b\n",
    "            # print(\"y_hat : \", y_hat.shape, y_hat)\n",
    "            \n",
    "            # classification\n",
    "            y_times_y_hat = np.multiply(y, y_hat)\n",
    "            # print(\"y_times_y_hat : \", y_times_y_hat.shape, y_times_y_hat)\n",
    "            \n",
    "            indicator_0_or_1 = np.where(y_times_y_hat <= 0, -1, 1)\n",
    "            # print(indicator_0_or_1)\n",
    "            \n",
    "            # if indicator_0_or_1 >= 1:\n",
    "            #     print(\"CORRECTLY CLASSIFIED POINTS\")\n",
    "            #     print(y_times_y_hat)\n",
    "            # else: \n",
    "            #     print(\"MISCLASSIFIED POINTS\")\n",
    "            #     print(y_times_y_hat)\n",
    "            \n",
    "            gradient = X.T * indicator_0_or_1 * (y_hat - y)\n",
    "            # print(\"gradient=\", gradient)\n",
    "            # print(X, \"*\", indicator_0_or_1, \"* (\", y_hat, \"-\", y, \") =\", gradient)\n",
    "            \n",
    "            # print(w.shape, \"+\", step_size, \"*\", X.T.shape, \"*\", y.shape)\n",
    "            w = np.add(w, (step_size * np.dot(X.T, y)))\n",
    "            # print(\"w : \", w.shape)\n",
    "            # print(b, \"+\", step_size, \"*\", y_hat, \"*\", y.shape)\n",
    "            b = b + (step_size * np.dot(y, y_hat))\n",
    "            print(\"b : \", b)\n",
    "           \n",
    "            # print()\n",
    "\n",
    "    elif loss == \"logistic\":\n",
    "        ################################################\n",
    "        # TODO 2 : perform \"max_iterations\" steps of   #\n",
    "        # gradient descent with step size \"step_size\"  #\n",
    "        # to minimize logistic loss                    # \n",
    "        ################################################\n",
    "\n",
    "        \n",
    "        pass\n",
    "    else:\n",
    "        raise \"Undefined loss function.\"\n",
    "\n",
    "    assert w.shape == (D,)\n",
    "    return w, b        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a702782-ac97-4c25-a19f-4683b35bf82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_predict(X, w, b):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - X: testing features, a N-by-D numpy array, where N is the \n",
    "    number of training points and D is the dimensionality of features\n",
    "    - w: D-dimensional vector, a numpy array which is the weight \n",
    "    vector of your learned model\n",
    "    - b: scalar, which is the bias of your model\n",
    "    \n",
    "    Returns:\n",
    "    - preds: N-dimensional vector of binary predictions (either 0 or 1)\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "        \n",
    "    #############################################################\n",
    "    # TODO 4 : predict DETERMINISTICALLY (i.e. do not randomize)#\n",
    "    #############################################################\n",
    "    \n",
    "    preds = np.dot(X, w) + b\n",
    "    assert preds.shape == (N,) \n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96941ded-9c4b-4e06-96ce-3e9c25adbf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = binary_toy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84dc446c-9d5a-4223-8997-ad75a05544b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# my_X_train = X_train[0:5]\n",
    "# my_y_train = y_train[0:5]\n",
    "# my_X_test = X_test[0:5]\n",
    "# my_y_test = y_test[0:5]\n",
    "\n",
    "# # print(my_X_train)\n",
    "# # print(my_y_train)\n",
    "# for loss_type in [\"perceptron\"]:\n",
    "#     my_w, my_b = binary_train(my_X_train, my_y_train, loss=loss_type)\n",
    "#     # print(my_w, my_b)\n",
    "#     my_train_preds = binary_predict(my_X_train, my_w, my_b)\n",
    "#     # print(my_train_preds)\n",
    "#     my_preds = binary_predict(my_X_test, my_w, my_b)\n",
    "#     # print(my_preds)\n",
    "#     print(loss_type + ' train acc: %f, test acc: %f' \n",
    "#                 %(accuracy_score(my_y_train, my_train_preds), accuracy_score(my_y_test, my_preds)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a85fa547-1563-4123-931f-0c59d5c212c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350 training points of size:  2\n",
      "b :  0.0\n",
      "b :  30198.60337889081\n",
      "b :  241588.8270311265\n",
      "b :  1540128.7723234314\n",
      "b :  9361567.047456156\n",
      "b :  56320395.301631376\n",
      "b :  338103563.4300615\n",
      "b :  2028832770.8040218\n",
      "b :  12173238213.651163\n",
      "b :  73039701069.33742\n",
      "b :  438238508402.0585\n",
      "b :  2629431382596.989\n",
      "b :  15776588657965.18\n",
      "b :  94659532340372.97\n",
      "b :  567957194465018.4\n",
      "b :  3407743167243090.0\n",
      "b :  2.0446459003941716e+16\n",
      "b :  1.2267875402416368e+17\n",
      "b :  7.360725241455259e+17\n",
      "b :  4.416435144873728e+18\n",
      "b :  2.6498610869242954e+19\n",
      "b :  1.589916652154581e+20\n",
      "b :  9.539499912927488e+20\n",
      "b :  5.723699947756493e+21\n",
      "b :  3.434219968653895e+22\n",
      "b :  2.060531981192337e+23\n",
      "b :  1.2363191887154022e+24\n",
      "b :  7.417915132292413e+24\n",
      "b :  4.450749079375448e+25\n",
      "b :  2.6704494476252683e+26\n",
      "b :  1.6022696685751611e+27\n",
      "b :  9.613618011450967e+27\n",
      "b :  5.7681708068705815e+28\n",
      "b :  3.460902484122348e+29\n",
      "b :  2.076541490473409e+30\n",
      "b :  1.2459248942840456e+31\n",
      "b :  7.475549365704274e+31\n",
      "b :  4.485329619422564e+32\n",
      "b :  2.6911977716535386e+33\n",
      "b :  1.6147186629921231e+34\n",
      "b :  9.68831197795274e+34\n",
      "b :  5.8129871867716444e+35\n",
      "b :  3.4877923120629867e+36\n",
      "b :  2.0926753872377922e+37\n",
      "b :  1.2556052323426751e+38\n",
      "b :  7.533631394056052e+38\n",
      "b :  4.5201788364336307e+39\n",
      "b :  2.712107301860179e+40\n",
      "b :  1.6272643811161073e+41\n",
      "b :  9.763586286696643e+41\n",
      "b :  5.858151772017986e+42\n",
      "b :  3.514891063210791e+43\n",
      "b :  2.1089346379264747e+44\n",
      "b :  1.2653607827558847e+45\n",
      "b :  7.592164696535307e+45\n",
      "b :  4.5552988179211847e+46\n",
      "b :  2.733179290752711e+47\n",
      "b :  1.6399075744516266e+48\n",
      "b :  9.83944544670976e+48\n",
      "b :  5.903667268025855e+49\n",
      "b :  3.5422003608155127e+50\n",
      "b :  2.1253202164893078e+51\n",
      "b :  1.2751921298935843e+52\n",
      "b :  7.651152779361507e+52\n",
      "b :  4.590691667616903e+53\n",
      "b :  2.754415000570142e+54\n",
      "b :  1.6526490003420849e+55\n",
      "b :  9.91589400205251e+55\n",
      "b :  5.949536401231506e+56\n",
      "b :  3.569721840738903e+57\n",
      "b :  2.141833104443342e+58\n",
      "b :  1.2850998626660049e+59\n",
      "b :  7.710599175996031e+59\n",
      "b :  4.626359505597619e+60\n",
      "b :  2.775815703358571e+61\n",
      "b :  1.6654894220151425e+62\n",
      "b :  9.992936532090853e+62\n",
      "b :  5.995761919254512e+63\n",
      "b :  3.5974571515527077e+64\n",
      "b :  2.158474290931624e+65\n",
      "b :  1.2950845745589745e+66\n",
      "b :  7.770507447353847e+66\n",
      "b :  4.6623044684123076e+67\n",
      "b :  2.7973826810473847e+68\n",
      "b :  1.6784296086284303e+69\n",
      "b :  1.0070577651770584e+70\n",
      "b :  6.042346591062352e+70\n",
      "b :  3.6254079546374114e+71\n",
      "b :  2.1752447727824464e+72\n",
      "b :  1.3051468636694677e+73\n",
      "b :  7.830881182016807e+73\n",
      "b :  4.698528709210085e+74\n",
      "b :  2.819117225526051e+75\n",
      "b :  1.691470335315631e+76\n",
      "b :  1.0148822011893786e+77\n",
      "b :  6.089293207136272e+77\n",
      "b :  3.653575924281763e+78\n",
      "b :  2.1921455545690575e+79\n",
      "b :  1.3152873327414343e+80\n",
      "b :  7.891723996448606e+80\n",
      "b :  4.735034397869164e+81\n",
      "b :  2.8410206387214985e+82\n",
      "b :  1.7046123832328988e+83\n",
      "b :  1.0227674299397395e+84\n",
      "b :  6.136604579638436e+84\n",
      "b :  3.6819627477830625e+85\n",
      "b :  2.2091776486698373e+86\n",
      "b :  1.3255065892019025e+87\n",
      "b :  7.953039535211415e+87\n",
      "b :  4.771823721126849e+88\n",
      "b :  2.863094232676109e+89\n",
      "b :  1.7178565396056655e+90\n",
      "b :  1.030713923763399e+91\n",
      "b :  6.184283542580396e+91\n",
      "b :  3.7105701255482376e+92\n",
      "b :  2.2263420753289424e+93\n",
      "b :  1.3358052451973655e+94\n",
      "b :  8.014831471184194e+94\n",
      "b :  4.808898882710516e+95\n",
      "b :  2.88533932962631e+96\n",
      "b :  1.7312035977757855e+97\n",
      "b :  1.0387221586654712e+98\n",
      "b :  6.232332951992828e+98\n",
      "b :  3.739399771195697e+99\n",
      "b :  2.2436398627174177e+100\n",
      "b :  1.3461839176304507e+101\n",
      "b :  8.077103505782705e+101\n",
      "b :  4.846262103469623e+102\n",
      "b :  2.9077572620817738e+103\n",
      "b :  1.744654357249064e+104\n",
      "b :  1.0467926143494384e+105\n",
      "b :  6.28075568609663e+105\n",
      "b :  3.768453411657978e+106\n",
      "b :  2.2610720469947866e+107\n",
      "b :  1.3566432281968717e+108\n",
      "b :  8.13985936918123e+108\n",
      "b :  4.8839156215087375e+109\n",
      "b :  2.9303493729052424e+110\n",
      "b :  1.7582096237431453e+111\n",
      "b :  1.0549257742458872e+112\n",
      "b :  6.329554645475322e+112\n",
      "b :  3.7977327872851936e+113\n",
      "b :  2.278639672371116e+114\n",
      "b :  1.3671838034226696e+115\n",
      "b :  8.203102820536017e+115\n",
      "b :  4.92186169232161e+116\n",
      "b :  2.9531170153929656e+117\n",
      "b :  1.7718702092357796e+118\n",
      "b :  1.0631221255414677e+119\n",
      "b :  6.3787327532488045e+119\n",
      "b :  3.827239651949283e+120\n",
      "b :  2.2963437911695703e+121\n",
      "b :  1.3778062747017422e+122\n",
      "b :  8.266837648210454e+122\n",
      "b :  4.9601025889262724e+123\n",
      "b :  2.976061553355763e+124\n",
      "b :  1.7856369320134584e+125\n",
      "b :  1.071382159208075e+126\n",
      "b :  6.42829295524845e+126\n",
      "b :  3.8569757731490707e+127\n",
      "b :  2.314185463889442e+128\n",
      "b :  1.3885112783336655e+129\n",
      "b :  8.331067670001993e+129\n",
      "b :  4.998640602001197e+130\n",
      "b :  2.999184361200718e+131\n",
      "b :  1.7995106167204308e+132\n",
      "b :  1.0797063700322585e+133\n",
      "b :  6.47823822019355e+133\n",
      "b :  3.88694293211613e+134\n",
      "b :  2.3321657592696782e+135\n",
      "b :  1.399299455561807e+136\n",
      "b :  8.395796733370843e+136\n",
      "b :  5.037478040022506e+137\n",
      "b :  3.0224868240135044e+138\n",
      "b :  1.8134920944081025e+139\n",
      "b :  1.0880952566448614e+140\n",
      "b :  6.528571539869169e+140\n",
      "b :  3.9171429239215006e+141\n",
      "b :  2.3502857543529e+142\n",
      "b :  1.4101714526117402e+143\n",
      "b :  8.461028715670441e+143\n",
      "b :  5.076617229402264e+144\n",
      "b :  3.045970337641358e+145\n",
      "b :  1.8275822025848154e+146\n",
      "b :  1.096549321550889e+147\n",
      "b :  6.579295929305336e+147\n",
      "b :  3.9475775575832013e+148\n",
      "b :  2.368546534549921e+149\n",
      "b :  1.4211279207299527e+150\n",
      "b :  8.526767524379715e+150\n",
      "b :  5.1160605146278294e+151\n",
      "b :  3.069636308776698e+152\n",
      "b :  1.8417817852660183e+153\n",
      "b :  1.105069071159611e+154\n",
      "b :  6.630414426957666e+154\n",
      "b :  3.978248656174599e+155\n",
      "b :  2.3869491937047593e+156\n",
      "b :  1.432169516222856e+157\n",
      "b :  8.593017097337135e+157\n",
      "b :  5.15581025840228e+158\n",
      "b :  3.0934861550413684e+159\n",
      "b :  1.856091693024821e+160\n",
      "b :  1.1136550158148927e+161\n",
      "b :  6.681930094889355e+161\n",
      "b :  4.009158056933614e+162\n",
      "b :  2.405494834160168e+163\n",
      "b :  1.4432969004961012e+164\n",
      "b :  8.659781402976605e+164\n",
      "b :  5.195868841785963e+165\n",
      "b :  3.1175213050715786e+166\n",
      "b :  1.870512783042947e+167\n",
      "b :  1.122307669825768e+168\n",
      "b :  6.733846018954607e+168\n",
      "b :  4.0403076113727645e+169\n",
      "b :  2.424184566823659e+170\n",
      "b :  1.454510740094195e+171\n",
      "b :  8.727064440565173e+171\n",
      "b :  5.236238664339103e+172\n",
      "b :  3.1417431986034617e+173\n",
      "b :  1.8850459191620775e+174\n",
      "b :  1.1310275514972465e+175\n",
      "b :  6.786165308983477e+175\n",
      "b :  4.0716991853900866e+176\n",
      "b :  2.443019511234052e+177\n",
      "b :  1.4658117067404312e+178\n",
      "b :  8.794870240442588e+178\n",
      "b :  5.276922144265552e+179\n",
      "b :  3.166153286559331e+180\n",
      "b :  1.8996919719355986e+181\n",
      "b :  1.1398151831613593e+182\n",
      "b :  6.838891098968156e+182\n",
      "b :  4.103334659380892e+183\n",
      "b :  2.4620007956285357e+184\n",
      "b :  1.477200477377121e+185\n",
      "b :  8.863202864262728e+185\n",
      "b :  5.317921718557636e+186\n",
      "b :  3.1907530311345816e+187\n",
      "b :  1.914451818680749e+188\n",
      "b :  1.1486710912084495e+189\n",
      "b :  6.892026547250696e+189\n",
      "b :  4.135215928350417e+190\n",
      "b :  2.4811295570102505e+191\n",
      "b :  1.4886777342061503e+192\n",
      "b :  8.932066405236902e+192\n",
      "b :  5.3592398431421414e+193\n",
      "b :  3.2155439058852857e+194\n",
      "b :  1.9293263435311717e+195\n",
      "b :  1.157595806118703e+196\n",
      "b :  6.945574836712217e+196\n",
      "b :  4.16734490202733e+197\n",
      "b :  2.500406941216398e+198\n",
      "b :  1.500244164729839e+199\n",
      "b :  9.001464988379035e+199\n",
      "b :  5.4008789930274204e+200\n",
      "b :  3.2405273958164524e+201\n",
      "b :  1.9443164374898715e+202\n",
      "b :  1.1665898624939228e+203\n",
      "b :  6.999539174963537e+203\n",
      "b :  4.199723504978122e+204\n",
      "b :  2.5198341029868733e+205\n",
      "b :  1.5119004617921242e+206\n",
      "b :  9.071402770752745e+206\n",
      "b :  5.442841662451647e+207\n",
      "b :  3.2657049974709877e+208\n",
      "b :  1.9594229984825924e+209\n",
      "b :  1.1756537990895555e+210\n",
      "b :  7.053922794537332e+210\n",
      "b :  4.232353676722399e+211\n",
      "b :  2.5394122060334395e+212\n",
      "b :  1.5236473236200635e+213\n",
      "b :  9.141883941720381e+213\n",
      "b :  5.485130365032228e+214\n",
      "b :  3.291078219019337e+215\n",
      "b :  1.9746469314116024e+216\n",
      "b :  1.1847881588469617e+217\n",
      "b :  7.108728953081769e+217\n",
      "b :  4.265237371849062e+218\n",
      "b :  2.5591424231094376e+219\n",
      "b :  1.5354854538656627e+220\n",
      "b :  9.212912723193974e+220\n",
      "b :  5.527747633916385e+221\n",
      "b :  3.3166485803498303e+222\n",
      "b :  1.989989148209898e+223\n",
      "b :  1.1939934889259388e+224\n",
      "b :  7.163960933555633e+224\n",
      "b :  4.298376560133381e+225\n",
      "b :  2.5790259360800284e+226\n",
      "b :  1.5474155616480173e+227\n",
      "b :  9.284493369888104e+227\n",
      "b :  5.5706960219328623e+228\n",
      "b :  3.3424176131597173e+229\n",
      "b :  2.0054505678958303e+230\n",
      "b :  1.2032703407374982e+231\n",
      "b :  7.21962204442499e+231\n",
      "b :  4.3317732266549937e+232\n",
      "b :  2.5990639359929966e+233\n",
      "b :  1.5594383615957982e+234\n",
      "b :  9.356630169574788e+234\n",
      "b :  5.613978101744874e+235\n",
      "b :  3.368386861046924e+236\n",
      "b :  2.0210321166281545e+237\n",
      "b :  1.2126192699768925e+238\n",
      "b :  7.275715619861355e+238\n",
      "b :  4.365429371916813e+239\n",
      "b :  2.6192576231500877e+240\n",
      "b :  1.5715545738900526e+241\n",
      "b :  9.429327443340316e+241\n",
      "b :  5.6575964660041885e+242\n",
      "b :  3.3945578796025135e+243\n",
      "b :  2.036734727761508e+244\n",
      "b :  1.222040836656905e+245\n",
      "b :  7.33224501994143e+245\n",
      "b :  4.399347011964859e+246\n",
      "b :  2.639608207178915e+247\n",
      "b :  1.583764924307349e+248\n",
      "b :  9.502589545844094e+248\n",
      "b :  5.701553727506456e+249\n",
      "b :  3.4209322365038737e+250\n",
      "b :  2.0525593419023244e+251\n",
      "b :  1.2315356051413946e+252\n",
      "b :  7.389213630848367e+252\n",
      "b :  4.433528178509021e+253\n",
      "b :  2.6601169071054127e+254\n",
      "b :  1.5960701442632476e+255\n",
      "b :  9.576420865579487e+255\n",
      "b :  5.745852519347692e+256\n",
      "b :  3.4475115116086155e+257\n",
      "b :  2.068506906965169e+258\n",
      "b :  1.2411041441791015e+259\n",
      "b :  7.446624865074608e+259\n",
      "b :  4.467974919044765e+260\n",
      "b :  2.6807849514268593e+261\n",
      "b :  1.6084709708561156e+262\n",
      "b :  9.650825825136692e+262\n",
      "b :  5.790495495082015e+263\n",
      "b :  3.4742972970492093e+264\n",
      "b :  2.0845783782295255e+265\n",
      "b :  1.250747026937715e+266\n",
      "b :  7.50448216162629e+266\n",
      "b :  4.502689296975775e+267\n",
      "b :  2.701613578185465e+268\n",
      "b :  1.620968146911279e+269\n",
      "b :  9.725808881467674e+269\n",
      "b :  5.835485328880605e+270\n",
      "b :  3.501291197328362e+271\n",
      "b :  2.1007747183970173e+272\n",
      "b :  1.2604648310382104e+273\n",
      "b :  7.562788986229261e+273\n",
      "b :  4.537673391737557e+274\n",
      "b :  2.7226040350425343e+275\n",
      "b :  1.6335624210255205e+276\n",
      "b :  9.801374526153123e+276\n",
      "b :  5.8808247156918744e+277\n",
      "b :  3.5284948294151243e+278\n",
      "b :  2.1170968976490745e+279\n",
      "b :  1.2702581385894446e+280\n",
      "b :  7.621548831536667e+280\n",
      "b :  4.5729292989220005e+281\n",
      "b :  2.7437575793532003e+282\n",
      "b :  1.6462545476119204e+283\n",
      "b :  9.877527285671522e+283\n",
      "b :  5.926516371402915e+284\n",
      "b :  3.5559098228417487e+285\n",
      "b :  2.133545893705049e+286\n",
      "b :  1.2801275362230294e+287\n",
      "b :  7.680765217338177e+287\n",
      "b :  4.608459130402906e+288\n",
      "b :  2.7650754782417434e+289\n",
      "b :  1.659045286945046e+290\n",
      "b :  9.954271721670275e+290\n",
      "b :  5.972563033002165e+291\n",
      "b :  3.5835378198012987e+292\n",
      "b :  2.150122691880779e+293\n",
      "b :  1.2900736151284674e+294\n",
      "b :  7.740441690770804e+294\n",
      "b :  4.644265014462482e+295\n",
      "b :  2.7865590086774898e+296\n",
      "b :  1.6719354052064937e+297\n",
      "b :  1.0031612431238965e+298\n",
      "b :  6.018967458743379e+298\n",
      "b :  3.6113804752460264e+299\n",
      "b :  2.166828285147616e+300\n",
      "b :  1.3000969710885693e+301\n",
      "b :  7.800581826531417e+301\n",
      "b :  4.68034909591885e+302\n",
      "b :  2.80820945755131e+303\n",
      "b :  1.6849256745307862e+304\n",
      "b :  1.0109554047184716e+305\n",
      "b :  6.06573242831083e+305\n",
      "b :  3.639439456986498e+306\n",
      "b :  2.1836636741918982e+307\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "b :  nan\n",
      "[173677.5344957   18370.04053505] nan\n",
      "perceptron train acc: 0.000000, test acc: 0.000000\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = binary_toy_data\n",
    "# print(X_train[0:5])\n",
    "# print(y_train[0:5])\n",
    "for loss_type in [\"perceptron\"]:\n",
    "    # print(loss_type)\n",
    "    w, b = binary_train(X_train, y_train, loss=loss_type)\n",
    "    print(w, b)\n",
    "    train_preds = binary_predict(X_train, w, b)\n",
    "    # print(train_preds)\n",
    "    preds = binary_predict(X_test, w, b)\n",
    "    # print(preds)\n",
    "    print(loss_type + ' train acc: %f, test acc: %f' \n",
    "                %(accuracy_score(y_train, train_preds), accuracy_score(y_test, preds)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c2c8f1bd-c2f2-4579-98be-7fd96e9d7f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_X_train = np.array([\n",
    "#             [2.7810836,2.550537003,0],\n",
    "#             [1.465489372,2.362125076,0],\n",
    "#             [3.396561688,4.400293529,0],\n",
    "#             [1.38807019,1.850220317,0],\n",
    "#             [3.06407232,3.005305973,0],\n",
    "#             [7.627531214,2.759262235,1],\n",
    "#             [5.332441248,2.088626775,1],\n",
    "#             [6.922596716,1.77106367,1],\n",
    "#             [8.675418651,-0.242068655,1],\n",
    "#             [7.673756466,3.508563011,1]\n",
    "#             ])\n",
    "\n",
    "# my_w_train = np.array([-0.1, 0.20653640140000007, -0.23418117710000003])\n",
    "\n",
    "# my_y_train = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n",
    "\n",
    "# for loss_type in [\"perceptron\"]:\n",
    "#     my_w, my_b = binary_train(my_X_train, my_y_train, loss=loss_type, w0=my_w_train)\n",
    "\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4345e65b-e714-4e6f-a29b-828effd6b002",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. USC CSCI-567 Machine Learning\n",
    "2. [make_classification](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html#sklearn-datasets-make-classification) Documentation\n",
    "3. [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split) Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4766ecdf-0cc3-4d86-bb74-7ce1f14fc0d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
