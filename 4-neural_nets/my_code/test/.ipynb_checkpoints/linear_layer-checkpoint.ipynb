{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79155b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import softmax_cross_entropy, add_momentum, data_loader_mnist, predict_label, DataSplit\n",
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccb36da",
   "metadata": {},
   "source": [
    "# doc string \n",
    "\n",
    "        The linear (affine/fully-connected) module.\n",
    "\n",
    "        It is built up with two arguments:\n",
    "        - input_D: the dimensionality of the input example/instance of the forward pass\n",
    "        - output_D: the dimensionality of the output example/instance of the forward pass\n",
    "\n",
    "        It has two learnable parameters:\n",
    "        - self.params['W']: the W matrix (numpy array) of shape input_D-by-output_D\n",
    "        - self.params['b']: the b vector (numpy array) of shape 1-by-output_D\n",
    "\n",
    "        It will record the partial derivatives of loss w.r.t. self.params['W'] and self.params['b'] in:\n",
    "        - self.gradient['W']: input_D-by-output_D numpy array\n",
    "        - self.gradient['b']: 1-by-output_D numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5e546f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. One linear Neural Network layer with forward and backward steps\n",
    "class linear_layer:\n",
    "\n",
    "\n",
    "    def __init__(self, input_D, output_D):\n",
    "\n",
    "        self.params = dict()\n",
    "        self.gradient = dict()\n",
    "\n",
    "        ###############################################################################################\n",
    "        # TODO: Use np.random.normal() with mean 0 and standard deviation 0.1 to initialize \n",
    "        #   - self.params['W'] \n",
    "        #   - self.params['b']\n",
    "        ###############################################################################################\n",
    "        \n",
    "        mu = 0 \n",
    "        sigma = 0.1\n",
    "        self.paramas['W'] = np.random.normal(mu, sigma)\n",
    "        self.params['b'] = np.random.normal(mu, sigma)\n",
    "        \n",
    "\n",
    "        ###############################################################################################\n",
    "        # TODO: Initialize the following two (gradients) with zeros\n",
    "        #   - self.gradient['W']\n",
    "        #   - self.gradient['b']\n",
    "        ###############################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        \"\"\"\n",
    "            The forward pass of the linear (affine/fully-connected) module.\n",
    "\n",
    "            Input:\n",
    "            - X: A N-by-input_D numpy array, where each 'row' is an input example/instance (N is the batch size)\n",
    "\n",
    "            Return:\n",
    "            - forward_output: A N-by-output_D numpy array, where each 'row' is an output example/instance.\n",
    "        \"\"\"\n",
    "\n",
    "        ################################################################################\n",
    "        # TODO: Implement the linear forward pass. Store the result in forward_output  #\n",
    "        ################################################################################\n",
    "\n",
    "        return forward_output\n",
    "\n",
    "    def backward(self, X, grad):\n",
    "\n",
    "        \"\"\"\n",
    "            The backward pass of the linear (affine/fully-connected) module.\n",
    "\n",
    "            Input:\n",
    "            - X: A N-by-input_D numpy array, the input to the forward pass.\n",
    "            - grad: A N-by-output_D numpy array, where each 'row' (say row i) is the partial derivative of the mini-batch loss\n",
    "                 w.r.t. forward_output[i].\n",
    "\n",
    "            Operation:\n",
    "            - Compute the partial derivatives (gradients) of the mini-batch loss w.r.t. self.params['W'], self.params['b'].\n",
    "            \n",
    "            Return:\n",
    "            - backward_output: A N-by-input_D numpy array, where each 'row' (say row i) is the partial derivatives of the mini-batch loss w.r.t. X[i].\n",
    "        \"\"\"\n",
    "\n",
    "        #################################################################################################\n",
    "        # TODO: Implement the backward pass (i.e., compute the following three terms)\n",
    "        #   - self.gradient['W'] (input_D-by-output_D numpy array, the gradient of the mini-batch loss w.r.t. self.params['W'])\n",
    "        #   - self.gradient['b'] (1-by-output_D numpy array, the gradient of the mini-batch loss w.r.t. self.params['b'])\n",
    "        #   - backward_output (N-by-input_D numpy array, the gradient of the mini-batch loss w.r.t. X)\n",
    "        # only return backward_output, but need to compute self.gradient['W'] and self.gradient['b']\n",
    "        #################################################################################################\n",
    "\n",
    "\n",
    "        return backward_output\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
